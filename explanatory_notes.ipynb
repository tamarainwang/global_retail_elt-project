{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries and Load Environment Variables:\n",
    "\n",
    "I first start by importing essential libraries such as os, pandas, dotenv, psycopg2, and openpyxl. These are crucial for my data manipulation, database interactions, and handling Excel files. I use load_dotenv to ensure my environment variables from the .env file are up to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sqlalchemy in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (2.0.24)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from sqlalchemy) (4.9.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install sqlalchemy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os #used to set my environment variables\n",
    "import pandas as pd # data analysis and manipulation library\n",
    "from dotenv import load_dotenv #  used to load environment variables from a .env file into my Python environment.\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2 # database adapter library that enables Python applications interact with PostgreSQL databases\n",
    "import openpyxl # Python library used for reading from and writing to Excel 2010+ files\n",
    "from datetime import datetime #important when working with dates\n",
    "\n",
    "load_dotenv(override=True) #this means that if we change the values of our .env file. it picks the new value and we dont have to reload it.\n",
    "\n",
    "pg_url = os.getenv('POSTGRES_URL') #  retrieves the value of my database url\n",
    "\n",
    "type(pg_url) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In the beginning....\n",
    "\n",
    "After importing the necessary libraries , the next logical step would be to explore ad understand the data I am working with. To do this , I need to read the excel file using pandas library which is shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/tamarainwang/Downloads/Week_6/hands_on_proj/global-superstore-data.xlsx'\n",
    "\n",
    "df = pd.read_excel(file_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploring the dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row ID</th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Ship Date</th>\n",
       "      <th>Ship Mode</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>Customer Name</th>\n",
       "      <th>Segment</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>...</th>\n",
       "      <th>Product ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sub-Category</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Profit</th>\n",
       "      <th>Shipping Cost</th>\n",
       "      <th>Order Priority</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32298</td>\n",
       "      <td>CA-2012-124891</td>\n",
       "      <td>2012-07-31</td>\n",
       "      <td>2012-07-31</td>\n",
       "      <td>Same Day</td>\n",
       "      <td>RH-19495</td>\n",
       "      <td>Rick Hansen</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>New York City</td>\n",
       "      <td>New York</td>\n",
       "      <td>...</td>\n",
       "      <td>TEC-AC-10003033</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Plantronics CS510 - Over-the-Head monaural Wir...</td>\n",
       "      <td>2309.650</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>762.1845</td>\n",
       "      <td>933.57</td>\n",
       "      <td>Critical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26341</td>\n",
       "      <td>IN-2013-77878</td>\n",
       "      <td>2013-02-05</td>\n",
       "      <td>2013-02-07</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>JR-16210</td>\n",
       "      <td>Justin Ritter</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>Wollongong</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>...</td>\n",
       "      <td>FUR-CH-10003950</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Chairs</td>\n",
       "      <td>Novimex Executive Leather Armchair, Black</td>\n",
       "      <td>3709.395</td>\n",
       "      <td>9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-288.7650</td>\n",
       "      <td>923.63</td>\n",
       "      <td>Critical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25330</td>\n",
       "      <td>IN-2013-71249</td>\n",
       "      <td>2013-10-17</td>\n",
       "      <td>2013-10-18</td>\n",
       "      <td>First Class</td>\n",
       "      <td>CR-12730</td>\n",
       "      <td>Craig Reiter</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Queensland</td>\n",
       "      <td>...</td>\n",
       "      <td>TEC-PH-10004664</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Phones</td>\n",
       "      <td>Nokia Smart Phone, with Caller ID</td>\n",
       "      <td>5175.171</td>\n",
       "      <td>9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>919.9710</td>\n",
       "      <td>915.49</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13524</td>\n",
       "      <td>ES-2013-1579342</td>\n",
       "      <td>2013-01-28</td>\n",
       "      <td>2013-01-30</td>\n",
       "      <td>First Class</td>\n",
       "      <td>KM-16375</td>\n",
       "      <td>Katherine Murray</td>\n",
       "      <td>Home Office</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>...</td>\n",
       "      <td>TEC-PH-10004583</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Phones</td>\n",
       "      <td>Motorola Smart Phone, Cordless</td>\n",
       "      <td>2892.510</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-96.5400</td>\n",
       "      <td>910.16</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47221</td>\n",
       "      <td>SG-2013-4320</td>\n",
       "      <td>2013-11-05</td>\n",
       "      <td>2013-11-06</td>\n",
       "      <td>Same Day</td>\n",
       "      <td>RH-9495</td>\n",
       "      <td>Rick Hansen</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>Dakar</td>\n",
       "      <td>Dakar</td>\n",
       "      <td>...</td>\n",
       "      <td>TEC-SHA-10000501</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Copiers</td>\n",
       "      <td>Sharp Wireless Fax, High-Speed</td>\n",
       "      <td>2832.960</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>311.5200</td>\n",
       "      <td>903.04</td>\n",
       "      <td>Critical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Row ID         Order ID Order Date  Ship Date     Ship Mode Customer ID  \\\n",
       "0   32298   CA-2012-124891 2012-07-31 2012-07-31      Same Day    RH-19495   \n",
       "1   26341    IN-2013-77878 2013-02-05 2013-02-07  Second Class    JR-16210   \n",
       "2   25330    IN-2013-71249 2013-10-17 2013-10-18   First Class    CR-12730   \n",
       "3   13524  ES-2013-1579342 2013-01-28 2013-01-30   First Class    KM-16375   \n",
       "4   47221     SG-2013-4320 2013-11-05 2013-11-06      Same Day     RH-9495   \n",
       "\n",
       "      Customer Name      Segment           City            State  ...  \\\n",
       "0       Rick Hansen     Consumer  New York City         New York  ...   \n",
       "1     Justin Ritter    Corporate     Wollongong  New South Wales  ...   \n",
       "2      Craig Reiter     Consumer       Brisbane       Queensland  ...   \n",
       "3  Katherine Murray  Home Office         Berlin           Berlin  ...   \n",
       "4       Rick Hansen     Consumer          Dakar            Dakar  ...   \n",
       "\n",
       "         Product ID    Category Sub-Category  \\\n",
       "0   TEC-AC-10003033  Technology  Accessories   \n",
       "1   FUR-CH-10003950   Furniture       Chairs   \n",
       "2   TEC-PH-10004664  Technology       Phones   \n",
       "3   TEC-PH-10004583  Technology       Phones   \n",
       "4  TEC-SHA-10000501  Technology      Copiers   \n",
       "\n",
       "                                        Product Name     Sales Quantity  \\\n",
       "0  Plantronics CS510 - Over-the-Head monaural Wir...  2309.650        7   \n",
       "1          Novimex Executive Leather Armchair, Black  3709.395        9   \n",
       "2                  Nokia Smart Phone, with Caller ID  5175.171        9   \n",
       "3                     Motorola Smart Phone, Cordless  2892.510        5   \n",
       "4                     Sharp Wireless Fax, High-Speed  2832.960        8   \n",
       "\n",
       "  Discount    Profit  Shipping Cost  Order Priority  \n",
       "0      0.0  762.1845         933.57        Critical  \n",
       "1      0.1 -288.7650         923.63        Critical  \n",
       "2      0.1  919.9710         915.49          Medium  \n",
       "3      0.1  -96.5400         910.16          Medium  \n",
       "4      0.0  311.5200         903.04        Critical  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Row ID', 'Order ID', 'Order Date', 'Ship Date', 'Ship Mode',\n",
       "       'Customer ID', 'Customer Name', 'Segment', 'City', 'State', 'Country',\n",
       "       'Postal Code', 'Market', 'Region', 'Product ID', 'Category',\n",
       "       'Sub-Category', 'Product Name', 'Sales', 'Quantity', 'Discount',\n",
       "       'Profit', 'Shipping Cost', 'Order Priority'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n",
    "\n",
    "#the column names need to be properly formatted. There are spaces and hyphens in the columns names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from the Excel file into a DataFrame\n",
    "\n",
    "Now that I have explored the data and seen that the column names need to be properly formatted(there are spaces and hyphens in the columns names). My work can begin. The first step would be be write a function that reads my global-superstore-data.xlsx (i.e excel file) into a pandas dataframe. The use of functions will help me write the logic once and then call the function whenever I need to perform that task in the defined function, rather than rewriting the same code again and again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_excel_data(data_source: str, sheet_name: str = None):\n",
    "    \"\"\"This function reads an Excel file from the specified path (data_source) and loads data into a Pandas DataFrame. \n",
    "    If a sheet name is provided, it loads data from that specific sheet. \n",
    "    If no sheet name is specified, it loads data from the first sheet by default\"\"\"\n",
    "    print(f'Loading data from {data_source}...')\n",
    "    if sheet_name:\n",
    "        raw_data = pd.read_excel(data_source, sheet_name=sheet_name)\n",
    "    else:\n",
    "        raw_data = pd.read_excel(data_source)\n",
    "    return raw_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation\n",
    "\n",
    "Next, I defined the function \"transform_column_names\" to format my dataFrame column names,removing leading or trailing spaces and making them lowercase . Then I replaced spaces and hyphens with underscores. This standardized the column names for database compatibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fxn to transform column names\n",
    "\n",
    "def transform_column_names(df:pd.DataFrame)-> pd.DataFrame:\n",
    "    \"\"\"iterates over each column name in the DataFrame's columns and transforms it to lowercase and replaces spaces with underscores\"\"\"\n",
    "    df.columns = [column.strip().lower().replace(' ','_').replace('-','_') for column in df.columns]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the transform_column_names function to iterates over & each column name in the dataframe (df) and then displayed all the columns to view confirm the transformation. The result is a transformed and standardised dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['row_id', 'order_id', 'order_date', 'ship_date', 'ship_mode',\n",
       "       'customer_id', 'customer_name', 'segment', 'city', 'state', 'country',\n",
       "       'postal_code', 'market', 'region', 'product_id', 'category',\n",
       "       'sub_category', 'product_name', 'sales', 'quantity', 'discount',\n",
       "       'profit', 'shipping_cost', 'order_priority'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_transformed = transform_column_names(df)\n",
    "\n",
    "raw_data_transformed.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to break the main dataframe (raw_data_transformed) into four distinct dataFrames i.e products, customers, locations, and orders. I defined  the normalize_data function to do this. This function breaks down the main dataFrame into four distinct subsets (also dataframes) and removes duplicates from these dataframes. I also included print statements to track the progress of the normalization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization ----> Breaking it all apart. Breaking the table into the 4 subsets below,chose the primary key in each subset and then remove exact duplicates \n",
    "\n",
    "# Products\n",
    "# customers\n",
    "# location\n",
    "# orders\n",
    "\n",
    "def normalize_data(df):\n",
    "    \"\"\"Function to normalize data into 4 dataframes ie 4 tables\"\"\"\n",
    "    print(f\"Normalization in progress.....current length = {len(df)}\")\n",
    "\n",
    "    products_df = df[['product_id', 'category', 'sub_category', 'product_name']]\n",
    "    products_df_clean = products_df.drop_duplicates()\n",
    "    print(f\"Done transforming Products ---> current length = {len(products_df_clean)}\")\n",
    "\n",
    "    customers_df = df[['customer_id', 'customer_name']]\n",
    "    customers_df_clean = customers_df.drop_duplicates()\n",
    "    print(f\"Done transforming customers ---> current length = {len(customers_df_clean)}\")\n",
    "\n",
    "    locations_df = df[['city', 'state', 'country','market',]]\n",
    "    locations_df_clean = locations_df.drop_duplicates()\n",
    "    print(f\"Done transforming locations ---> current length = {len(locations_df_clean)}\")\n",
    "      \n",
    "    # The next thing to do would be to create my orders_df and remove exact duplicates based on a primary key i.e order_id\n",
    "    # However I noticed that the order_id would not make a suitable primary key for the orders df because the order id was not unique.\n",
    "    # This was because each order had multiple products per order causing a duplication inthe order_id\n",
    "    # then to drop duplicates in the orders df based on the row_id which is now the  primary key \n",
    "\n",
    "    orders_df = raw_data_transformed.drop(columns = ['category', 'sub_category', 'product_name','customer_name','state', 'city'])\n",
    "\n",
    "    print(f'Length of orders_df: {len(orders_df)}')\n",
    "\n",
    "    orders_df_clean = orders_df.drop_duplicates(subset = ['row_id'])\n",
    "\n",
    "    print(len(orders_df_clean))\n",
    "\n",
    "    print(f\"Done transforming orders ---> current length = {len(orders_df_clean)}\")\n",
    "\n",
    "    return[products_df_clean,customers_df_clean,locations_df_clean,orders_df]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Data to CSV Files\n",
    "\n",
    "I then wrote my transformed dataFrames into CSV files in the outputs/models directory (an intermediate storage) and added a quick print statement to confirm that the I've successfully saved the csv files to the directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_to_csv(list_of_dfs:list):\n",
    "    \"\"\"Function to write a list of dataframes to a csv file.\"\"\"\n",
    "\n",
    "    #write dfs to csv files\n",
    "\n",
    "    output_dir = \"outputs/models/\"\n",
    "    list_of_dfs[0].to_csv(f\"{output_dir}products.csv\", index=False)\n",
    "    list_of_dfs[1].to_csv(f\"{output_dir}customers.csv\", index=False)\n",
    "    list_of_dfs[2].to_csv(f\"{output_dir}locations.csv\", index=False)\n",
    "    list_of_dfs[3].to_csv(f\"{output_dir}orders.csv\", index=False)\n",
    "\n",
    "    print(f\"{len(list_of_dfs)} Files written to intermediate storage.\")\n",
    "\n",
    "    print(\"Files written to intermediate storage\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading CSV files to elephantsql database while adding a column showing when the data was uploaded to the elephantSQL database\n",
    "\n",
    "Now that i have my normalised subsets in my intermediate storage (i.e \"outputs/models/\"). The next step would be to load the normalised subsets to my final storage (ElephantSQL database in this instance).However I need to add a crucial column to my tables. This column is a timestamp column called \"_elt_loaded_at\" which indicates time data was added to the database. The logic after this would be to immediately load the timestamped dataframes to the database.\n",
    "\n",
    "To achieve this, I defined 2 functions.The first to modify my timestamped table in my intermediate storage and the 2nd reads a CSV from 'outputs/models/{table}.csv', adds a timestamp column(the first function that adds a timestamp column is called here ), and writes the data to the specified database table. If the table exists, it is replaced. This function uses the 'POSTGRES_URL' from environment variables to connect to the database.\n",
    "The 2nd function reads "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1st function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_dataframe(original_data:pd.DataFrame)-> pd.DataFrame:\n",
    "    \"\"\"This function mofifies our exisiting dataframe by \n",
    "    adding a timestamp column to return a new dataframe.\"\"\"\n",
    "    # this gets the current time using datetime module imported above ,formats it and is stored as current time\n",
    "    current_time = datetime.now().strftime('%Y-%d-%m %H:%M:%S') \n",
    "    # this adds a new column to original_data dataframe called \"_elt_loaded_at\" & every row in this new column is filled with the value of current_time.\n",
    "    original_data['_elt_loaded_at'] = current_time\n",
    "    return original_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2nd function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_to_db(table:str):\n",
    "    \"\"\"\n",
    "    Loads data from a CSV file into a specified table in a database, with an added timestamp column.\n",
    "\n",
    "    This function performs the following steps:\n",
    "    1. Reads a CSV file corresponding to the given table name from the 'outputs/models' directory into a DataFrame.\n",
    "    2. Adds a timestamp column to this DataFrame, marking the time at which the data was processed.\n",
    "    3. Connects to a database using the URL specified in the 'POSTGRES_URL' environment variable.\n",
    "    4. Writes the DataFrame to the specified table in the database. If the table already exists, it is replaced.\n",
    "    5. Closes the database connection and prints a confirmation message.\n",
    "\n",
    "    Parameters:\n",
    "    table (str): The name of the table, which is also used to identify the CSV file to be loaded (assumed to be 'table.csv' in 'outputs/models').\n",
    "\n",
    "    Note:\n",
    "    The function requires 'pd', 'create_engine' from 'sqlalchemy', and 'datetime' to be imported and 'pg_url' to be defined prior to its call.\n",
    "    The function assumes that the environment variable 'POSTGRES_URL' is set and accessible.\n",
    "    \"\"\"\n",
    "  \n",
    "    df_without_timestamp = pd.read_csv(f\"outputs/models/{table}.csv\")\n",
    " \n",
    "    timestamped_dataframe = update_dataframe(original_data = df_without_timestamp)\n",
    "    engine = create_engine(pg_url)\n",
    "    connection = engine.connect()\n",
    "    timestamped_dataframe.to_sql(table, con=connection, if_exists='replace', index=False)\n",
    "    connection.close()\n",
    "    print(f\"Data loaded to {table} table.\")\n",
    "\n",
    "# It is important to note that the timestamp is added to the DataFrame just before it's written to the database. In an ideal and efficient environment, this should minimize the delay between the timestamp generation and the timestamped dataframes loading into the database.\n",
    "    \n",
    "# However,there are a few scenarios where a delay might still occur due to large volumes of data, database server's load, network latency,database server processing etc. In these instances database-level timestamping is ideal.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bringing it together ...An Excel to Database ETL Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ ==\"__main__\":\n",
    "\n",
    "    #load data\n",
    "    raw_data = load_excel_data(data_source=file_path,sheet_name=\"Orders\")\n",
    "    raw_data_transformed = transform_column_names(raw_data)\n",
    "\n",
    "    #normalize data\n",
    "    normalized_datasets_list = normalize_data(df=raw_data_transformed)\n",
    "\n",
    "    #write_to_csv\n",
    "    write_to_csv(normalized_datasets_list)\n",
    "\n",
    "    #load data to db\n",
    "    files_to_load = [\"products\",\"customers\",\"locations\",\"orders\"]\n",
    "    for file in files_to_load:\n",
    "        load_data_to_db(table=file)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Analytics tables \n",
    "\n",
    "4 tables where loaded into our database (kindly refer to snapshots_of_table_in_db ). However the company requires insights into their best selling items, clients and locations.\n",
    "\n",
    "This is achieved by creating 2 analytics tables\n",
    "\n",
    "Top 100 products\n",
    "Top 20 customers\n",
    "Top 3 markets\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
