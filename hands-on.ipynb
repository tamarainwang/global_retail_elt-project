{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries and Load Environment Variables:\n",
    "\n",
    "I first start by importing essential libraries such as os, pandas, dotenv, psycopg2, and openpyxl. These are crucial for my data manipulation, database interactions, and handling Excel files. I use load_dotenv to ensure my environment variables from the .env file are up to date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import pandas as pd # data analysis and manipulation library\n",
    "from dotenv import load_dotenv\n",
    "import psycopg2 # database adapter library that enables Python applications interact with PostgreSQL databases\n",
    "import openpyxl # Python library used for reading from and writing to Excel 2010+ files\n",
    "\n",
    "load_dotenv(override=True) #this means that if we change the values of our .env file. it picks the new value and we dont have to reload it.\n",
    "\n",
    "pg_url = os.getenv('POSTGRES_URL')\n",
    "\n",
    "type(pg_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Data from Excel: \n",
    "\n",
    "After importing the necessary libraries , the next step is to read the data into pandas.  I then specify and load an Excel file 'global-superstore-data.xlsx' into a pandas DataFrame, which kicks off my Extract phase of the ETL process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/Users/tamarainwang/Downloads/Week_6/hands_on_proj/global-superstore-data.xlsx'\n",
    "\n",
    "df = pd.read_excel(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Row ID</th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Ship Date</th>\n",
       "      <th>Ship Mode</th>\n",
       "      <th>Customer ID</th>\n",
       "      <th>Customer Name</th>\n",
       "      <th>Segment</th>\n",
       "      <th>City</th>\n",
       "      <th>State</th>\n",
       "      <th>...</th>\n",
       "      <th>Product ID</th>\n",
       "      <th>Category</th>\n",
       "      <th>Sub-Category</th>\n",
       "      <th>Product Name</th>\n",
       "      <th>Sales</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Discount</th>\n",
       "      <th>Profit</th>\n",
       "      <th>Shipping Cost</th>\n",
       "      <th>Order Priority</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>32298</td>\n",
       "      <td>CA-2012-124891</td>\n",
       "      <td>2012-07-31</td>\n",
       "      <td>2012-07-31</td>\n",
       "      <td>Same Day</td>\n",
       "      <td>RH-19495</td>\n",
       "      <td>Rick Hansen</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>New York City</td>\n",
       "      <td>New York</td>\n",
       "      <td>...</td>\n",
       "      <td>TEC-AC-10003033</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Accessories</td>\n",
       "      <td>Plantronics CS510 - Over-the-Head monaural Wir...</td>\n",
       "      <td>2309.650</td>\n",
       "      <td>7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>762.1845</td>\n",
       "      <td>933.57</td>\n",
       "      <td>Critical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>26341</td>\n",
       "      <td>IN-2013-77878</td>\n",
       "      <td>2013-02-05</td>\n",
       "      <td>2013-02-07</td>\n",
       "      <td>Second Class</td>\n",
       "      <td>JR-16210</td>\n",
       "      <td>Justin Ritter</td>\n",
       "      <td>Corporate</td>\n",
       "      <td>Wollongong</td>\n",
       "      <td>New South Wales</td>\n",
       "      <td>...</td>\n",
       "      <td>FUR-CH-10003950</td>\n",
       "      <td>Furniture</td>\n",
       "      <td>Chairs</td>\n",
       "      <td>Novimex Executive Leather Armchair, Black</td>\n",
       "      <td>3709.395</td>\n",
       "      <td>9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-288.7650</td>\n",
       "      <td>923.63</td>\n",
       "      <td>Critical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25330</td>\n",
       "      <td>IN-2013-71249</td>\n",
       "      <td>2013-10-17</td>\n",
       "      <td>2013-10-18</td>\n",
       "      <td>First Class</td>\n",
       "      <td>CR-12730</td>\n",
       "      <td>Craig Reiter</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>Brisbane</td>\n",
       "      <td>Queensland</td>\n",
       "      <td>...</td>\n",
       "      <td>TEC-PH-10004664</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Phones</td>\n",
       "      <td>Nokia Smart Phone, with Caller ID</td>\n",
       "      <td>5175.171</td>\n",
       "      <td>9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>919.9710</td>\n",
       "      <td>915.49</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13524</td>\n",
       "      <td>ES-2013-1579342</td>\n",
       "      <td>2013-01-28</td>\n",
       "      <td>2013-01-30</td>\n",
       "      <td>First Class</td>\n",
       "      <td>KM-16375</td>\n",
       "      <td>Katherine Murray</td>\n",
       "      <td>Home Office</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>...</td>\n",
       "      <td>TEC-PH-10004583</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Phones</td>\n",
       "      <td>Motorola Smart Phone, Cordless</td>\n",
       "      <td>2892.510</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-96.5400</td>\n",
       "      <td>910.16</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47221</td>\n",
       "      <td>SG-2013-4320</td>\n",
       "      <td>2013-11-05</td>\n",
       "      <td>2013-11-06</td>\n",
       "      <td>Same Day</td>\n",
       "      <td>RH-9495</td>\n",
       "      <td>Rick Hansen</td>\n",
       "      <td>Consumer</td>\n",
       "      <td>Dakar</td>\n",
       "      <td>Dakar</td>\n",
       "      <td>...</td>\n",
       "      <td>TEC-SHA-10000501</td>\n",
       "      <td>Technology</td>\n",
       "      <td>Copiers</td>\n",
       "      <td>Sharp Wireless Fax, High-Speed</td>\n",
       "      <td>2832.960</td>\n",
       "      <td>8</td>\n",
       "      <td>0.0</td>\n",
       "      <td>311.5200</td>\n",
       "      <td>903.04</td>\n",
       "      <td>Critical</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Row ID         Order ID Order Date  Ship Date     Ship Mode Customer ID  \\\n",
       "0   32298   CA-2012-124891 2012-07-31 2012-07-31      Same Day    RH-19495   \n",
       "1   26341    IN-2013-77878 2013-02-05 2013-02-07  Second Class    JR-16210   \n",
       "2   25330    IN-2013-71249 2013-10-17 2013-10-18   First Class    CR-12730   \n",
       "3   13524  ES-2013-1579342 2013-01-28 2013-01-30   First Class    KM-16375   \n",
       "4   47221     SG-2013-4320 2013-11-05 2013-11-06      Same Day     RH-9495   \n",
       "\n",
       "      Customer Name      Segment           City            State  ...  \\\n",
       "0       Rick Hansen     Consumer  New York City         New York  ...   \n",
       "1     Justin Ritter    Corporate     Wollongong  New South Wales  ...   \n",
       "2      Craig Reiter     Consumer       Brisbane       Queensland  ...   \n",
       "3  Katherine Murray  Home Office         Berlin           Berlin  ...   \n",
       "4       Rick Hansen     Consumer          Dakar            Dakar  ...   \n",
       "\n",
       "         Product ID    Category Sub-Category  \\\n",
       "0   TEC-AC-10003033  Technology  Accessories   \n",
       "1   FUR-CH-10003950   Furniture       Chairs   \n",
       "2   TEC-PH-10004664  Technology       Phones   \n",
       "3   TEC-PH-10004583  Technology       Phones   \n",
       "4  TEC-SHA-10000501  Technology      Copiers   \n",
       "\n",
       "                                        Product Name     Sales Quantity  \\\n",
       "0  Plantronics CS510 - Over-the-Head monaural Wir...  2309.650        7   \n",
       "1          Novimex Executive Leather Armchair, Black  3709.395        9   \n",
       "2                  Nokia Smart Phone, with Caller ID  5175.171        9   \n",
       "3                     Motorola Smart Phone, Cordless  2892.510        5   \n",
       "4                     Sharp Wireless Fax, High-Speed  2832.960        8   \n",
       "\n",
       "  Discount    Profit  Shipping Cost  Order Priority  \n",
       "0      0.0  762.1845         933.57        Critical  \n",
       "1      0.1 -288.7650         923.63        Critical  \n",
       "2      0.1  919.9710         915.49          Medium  \n",
       "3      0.1  -96.5400         910.16          Medium  \n",
       "4      0.0  311.5200         903.04        Critical  \n",
       "\n",
       "[5 rows x 24 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Row ID', 'Order ID', 'Order Date', 'Ship Date', 'Ship Mode',\n",
       "       'Customer ID', 'Customer Name', 'Segment', 'City', 'State', 'Country',\n",
       "       'Postal Code', 'Market', 'Region', 'Product ID', 'Category',\n",
       "       'Sub-Category', 'Product Name', 'Sales', 'Quantity', 'Discount',\n",
       "       'Profit', 'Shipping Cost', 'Order Priority'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns\n",
    "\n",
    "#the column names need to be properly formatted. There are spaces and hyphens in the columns names."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation\n",
    "\n",
    "After exploring the data frame , I defined the function \"transform_column_names\" to format my dataFrame column names,removing leading or trailing spaces and making them lowercase . Then replacing spaces and hyphens with underscores. This standardizes the column names for database compatibility.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fxn to transform column names\n",
    "\n",
    "def transform_column_names(df:pd.DataFrame)-> pd.DataFrame:\n",
    "    \"\"\"iterates over each column name in the DataFrame's columns and transforms it to lowercase and replaces spaces with underscores\"\"\"\n",
    "    df.columns = [column.strip().lower().replace(' ','_').replace('-','_') for column in df.columns]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I used the transform_column_names function to iterate over and transform each column name in the dataframe (df). The result is a trnasformed and standardised dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['row_id', 'order_id', 'order_date', 'ship_date', 'ship_mode',\n",
       "       'customer_id', 'customer_name', 'segment', 'city', 'state', 'country',\n",
       "       'postal_code', 'market', 'region', 'product_id', 'category',\n",
       "       'sub_category', 'product_name', 'sales', 'quantity', 'discount',\n",
       "       'profit', 'shipping_cost', 'order_priority'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_data_transformed = transform_column_names(df)\n",
    "\n",
    "raw_data_transformed.columns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to break the main dataframe into four distinct DataFrames i.e products, customers, locations, and orders. I defined  the normalize_data function to do this. This function breaks down the main DataFrame into four distinct dataFrame and made sure to remove duplicates from these dataFrames. I also included print statements to track the progress of this normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalization in progress.....current length = 51290\n",
      "Done transforming Products ---> current length = 10768\n",
      "Done transforming customers ---> current length = 1590\n",
      "Done transforming locations ---> current length = 3812\n"
     ]
    }
   ],
   "source": [
    "# Normalization ----> Breaking it all apart. Breaking the table into the 4 subsets below and then removing duplicates based on the primary key\n",
    "\n",
    "# Products\n",
    "# customers\n",
    "# location\n",
    "# orders\n",
    "\n",
    "print(f\"Normalization in progress.....current length = {len(raw_data_transformed)}\")\n",
    "\n",
    "products_df = raw_data_transformed[['product_id', 'category', 'sub_category', 'product_name']]\n",
    "products_df_clean = products_df.drop_duplicates()\n",
    "\n",
    "print(f\"Done transforming Products ---> current length = {len(products_df_clean)}\")\n",
    "\n",
    "customers_df = raw_data_transformed[['customer_id', 'customer_name']]\n",
    "customers_df_clean = customers_df.drop_duplicates()\n",
    "\n",
    "print(f\"Done transforming customers ---> current length = {len(customers_df_clean)}\")\n",
    "\n",
    "locations_df = raw_data_transformed[['city', 'state', 'country']]\n",
    "locations_df_clean = locations_df.drop_duplicates()\n",
    "\n",
    "print(f\"Done transforming locations ---> current length = {len(locations_df_clean)}\")\n",
    "\n",
    "# The next thing to do would be to create my orders_df and remove duplicates based on a primary key i.e order_id\n",
    "# However I noticed that the order_id would not make a suitable primary key for the orders df because the order id was not unique.\n",
    "# This was because each order had multiple products per order. So we have different \n",
    "# So I created the orders_df to exclude all other fields \n",
    "\n",
    "orders_df = raw_data_transformed.drop(columns = ['category', 'sub_category', 'product_name','customer_name','state', 'country'])\n",
    "\n",
    "print(len(orders_df))\n",
    "\n",
    "# Therefore I decided to drop duplicates in the orders df based on the row_id which is the primark key for \n",
    "\n",
    "orders_df_clean = orders_df.drop_duplicates(subset = ['row_id'])\n",
    "\n",
    "print(len(orders_df_clean))\n",
    "\n",
    "print(f\"Done transforming orders ---> current length = {len(orders_df_clean)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Data to CSV Files\n",
    "\n",
    "I then wrote my transformed dataFrames into CSV files in the outputs/models directory and added a quick print statement to confirm that the  I've successfully saved the csv files to the outputs/models directory (an intermediate storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files written to intermediate storage\n"
     ]
    }
   ],
   "source": [
    "#write dfs to csv files\n",
    "\n",
    "output_dir = \"hands_on_proj\"\n",
    "products_df.to_csv(f\"{output_dir}products.csv\", index=False)\n",
    "customers_df.to_csv(f\"{output_dir}customers.csv\", index=False)\n",
    "locations_df.to_csv(f\"{output_dir}locations.csv\", index=False)\n",
    "orders_df.to_csv(f\"{output_dir}orders.csv\", index=False)\n",
    "\n",
    "print(\"Files written to intermediate storage\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51290\n",
      "51290\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-09-12 11:43:45\n"
     ]
    }
   ],
   "source": [
    "print (datetime.now().strftime('%Y-%d-%m %H:%M:%S'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
